{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc801d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:162: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:188: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:162: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:188: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Star\\AppData\\Local\\Temp\\ipykernel_31492\\3804590652.py:162: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
      "C:\\Users\\Star\\AppData\\Local\\Temp\\ipykernel_31492\\3804590652.py:188: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  return re.sub('\\s+',' ',text)\n",
      "C:\\Users\\Star\\AppData\\Local\\Temp\\ipykernel_31492\\3804590652.py:162: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
      "C:\\Users\\Star\\AppData\\Local\\Temp\\ipykernel_31492\\3804590652.py:188: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  return re.sub('\\s+',' ',text)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# !pip3 install swifter\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# !pip3 install PySastrawi\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer, CountVectorizer\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "# !pip3 install swifter\n",
    "# !pip3 install PySastrawi\n",
    "\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "app = Flask(__name__)\n",
    "\n",
    "# API KNN\n",
    "\n",
    "@app.route('/knn')\n",
    "def KNN():\n",
    "    df = pd.read_csv(\"hasil_vector_matrix.csv\", header=None)\n",
    "\n",
    "    X = df.iloc[1:, :-1].values\n",
    "    y = df.iloc[1:, -2].values\n",
    "    XNum = X.astype(np.float64)\n",
    "    yNum = y.astype(np.float64)\n",
    "\n",
    "    similarities = cosine_similarity(XNum, XNum)\n",
    "\n",
    "    k = 3\n",
    "\n",
    "    nearest_neighbors = np.zeros((len(XNum), k), dtype=int)\n",
    "\n",
    "    for i in range(len(XNum)):\n",
    "        nearest_indices = np.argsort(similarities[i])[-k-1:-1][::-1]\n",
    "        nearest_neighbors[i] = yNum[nearest_indices]\n",
    "\n",
    "    predictions = np.zeros(len(XNum), dtype=int)\n",
    "\n",
    "    for i in range(len(XNum)):\n",
    "        prediction = np.argmax(np.bincount(nearest_neighbors[i]))\n",
    "        predictions[i] = prediction\n",
    "\n",
    "    accuracy = np.mean(predictions == yNum)\n",
    "    ranking = np.argsort(similarities, axis=1)[:, ::-1]\n",
    "\n",
    "    return jsonify(accuracy=accuracy, ranking=ranking.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# API tambah tweet to CSV\n",
    "@app.route('/add-tweet', methods=['POST'])\n",
    "def addTweet():\n",
    "    data = request.form\n",
    "    \n",
    "    df = pd.read_csv('data_tweet.csv', encoding='latin1')\n",
    "\n",
    "    current_rows = df.shape[0]\n",
    "    \n",
    "    tweet_text = data['tweet']\n",
    "    \n",
    "    df.loc[current_rows, 'rawContent'] = tweet_text\n",
    "    df.loc[current_rows, 'status'] = 0\n",
    "\n",
    "    df.to_csv('data_tweet.csv', index=False)\n",
    "    \n",
    "    return \"berhasil ditambahkan\" \n",
    "\n",
    "# mencari persentase untuk hasil negatif dan positif\n",
    "@app.route('/get-persentase-sentimen', methods=['GET'])\n",
    "def getPersentaseSentimen():\n",
    "    # Membaca file Excel\n",
    "    df = pd.read_csv('pelabelan.csv')\n",
    "\n",
    "    # Ambil kolom \"sentimen\"\n",
    "    sentimen = df['sentimen']\n",
    "\n",
    "    # Hitung jumlah prediksi positif dan negatif\n",
    "    positive_count = sentimen[sentimen == 'positif'].count()\n",
    "    negative_count = sentimen[sentimen == 'negatif'].count()\n",
    "\n",
    "    # Hitung persentase perbandingan antara prediksi positif dan negatif\n",
    "    total_count = len(sentimen)\n",
    "    positive_percentage = (positive_count / total_count) * 100\n",
    "    negative_percentage = (negative_count / total_count) * 100\n",
    "\n",
    "    # Tampilkan hasil persentase perbandingan\n",
    "\n",
    "    return jsonify({\"Persentase_positif\": positive_percentage, \"Persentase_negatif\": negative_percentage})\n",
    "\n",
    "# API data training\n",
    "@app.route('/data-training')\n",
    "def dataTraining():\n",
    "    TWEET_DATA = pd.read_csv(\"data_tweet.csv\", usecols=['rawContent', 'status'])\n",
    "    filtered_data = TWEET_DATA[TWEET_DATA['status'] == 1]\n",
    "    data = filtered_data['rawContent'].to_list()\n",
    "    \n",
    "    return jsonify(data)\n",
    "\n",
    "# API data testing\n",
    "@app.route('/data-testing')\n",
    "def dataTesting():\n",
    "    TWEET_DATA = pd.read_csv(\"data_tweet.csv\", usecols=['rawContent', 'status'])\n",
    "    filtered_data1 = TWEET_DATA.reset_index()\n",
    "    filtered_data1['status'] = filtered_data1['status'].replace(0.0, 0).astype(int)\n",
    "    sorted_data1 = pd.concat([\n",
    "        filtered_data1[filtered_data1['status'] == 0].sort_values(by='status', ascending=False),\n",
    "        filtered_data1[filtered_data1['status'] != 0]\n",
    "    ])\n",
    "    data1 = sorted_data1.to_dict(orient='records')\n",
    "\n",
    "    PELABELAN_DATA = pd.read_csv(\"pelabelan.csv\", usecols=['sentimen', 'status','aktual'])\n",
    "    filtered_data2 = PELABELAN_DATA.reset_index()\n",
    "    filtered_data2['status'] = filtered_data2['status'].replace(0.0, 0).astype(int)\n",
    "    sorted_data2 = pd.concat([\n",
    "        filtered_data2[filtered_data2['status'] == 0].sort_values(by='status', ascending=False),\n",
    "        filtered_data2[filtered_data2['status'] != 0]\n",
    "    ])\n",
    "    data2 = sorted_data2.to_dict(orient='records')\n",
    "\n",
    "    return jsonify({\"data_tweet\": data1, \"data_pelabelan\": data2})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# API Count data\n",
    "@app.route('/data-chart')\n",
    "def dataChart():\n",
    "    \n",
    "    TWEET_DATA = pd.read_csv(\"pelabelan.csv\", usecols=['sentimen'])\n",
    "    counts = TWEET_DATA['sentimen'].value_counts()\n",
    "    data = {\"positif\": int(counts.get('positif', 0)), \"negatif\": int(counts.get('negatif', 0))}\n",
    "    \n",
    "    return jsonify(data)\n",
    "\n",
    "\n",
    "# API preprocessing\n",
    "@app.route('/preprocessing')\n",
    "def preprocessing():\n",
    "    \n",
    "    TWEET_DATA = pd.read_csv(\"data_tweet.csv\")\n",
    "    \n",
    "    # mengubah text menjadi lowercase\n",
    "    TWEET_DATA['rawContent'] = TWEET_DATA['rawContent'].str.lower()\n",
    "\n",
    "    # ------ Tokenizing ---------\n",
    "\n",
    "    def remove_tweet_special(text):\n",
    "        # remove tab, new line, ans back slice\n",
    "        text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "        # remove non ASCII (emoticon, chinese word, .etc)\n",
    "        text = text.encode('ascii', 'replace').decode('ascii')\n",
    "        # remove mention, link, hashtag\n",
    "        text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "        # remove incomplete URL\n",
    "        return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "    TWEET_DATA['rawContent'] = TWEET_DATA['rawContent'].apply(remove_tweet_special)\n",
    "\n",
    "    #remove number\n",
    "    def remove_number(text):\n",
    "        return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    TWEET_DATA['rawContent'] = TWEET_DATA['rawContent'].apply(remove_number)\n",
    "\n",
    "    #remove punctuation\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "    TWEET_DATA['rawContent'] = TWEET_DATA['rawContent'].apply(remove_punctuation)\n",
    "\n",
    "    #remove whitespace leading & trailing\n",
    "    def remove_whitespace_LT(text):\n",
    "        return text.strip()\n",
    "\n",
    "    TWEET_DATA['rawContent'] = TWEET_DATA['rawContent'].apply(remove_whitespace_LT)\n",
    "\n",
    "    #remove multiple whitespace into single whitespace\n",
    "    def remove_whitespace_multiple(text):\n",
    "        return re.sub('\\s+',' ',text)\n",
    "\n",
    "    TWEET_DATA['rawContent'] = TWEET_DATA['rawContent'].apply(remove_whitespace_multiple)\n",
    "\n",
    "    # remove single char\n",
    "    def remove_singl_char(text):\n",
    "        return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "    TWEET_DATA['rawContent'] = TWEET_DATA['rawContent'].apply(remove_singl_char)\n",
    "\n",
    "    # NLTK word rokenize \n",
    "    def word_tokenize_wrapper(text):\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    TWEET_DATA['tweet_tokens'] = TWEET_DATA['rawContent'].apply(word_tokenize_wrapper)\n",
    "    \n",
    "    # NLTK calc frequency distribution\n",
    "    def freqDist_wrapper(text):\n",
    "        return FreqDist(text)\n",
    "\n",
    "    TWEET_DATA['tweet_tokens_fdist'] = TWEET_DATA['tweet_tokens'].apply(freqDist_wrapper)\n",
    "    TWEET_DATA['tweet_tokens_fdist'].head().apply(lambda x : x.most_common())\n",
    "    \n",
    "\n",
    "    # ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "    # get stopword indonesia\n",
    "    list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "    # ---------------------------- manualy add stopword  ------------------------------------\n",
    "    # append additional stopword\n",
    "    list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                           'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                           'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                           'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                           'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                           'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                           '&amp', 'yah'])\n",
    "\n",
    "    # ----------------------- add stopword from txt file ------------------------------------\n",
    "    # read txt stopword using pandas\n",
    "    txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "    # convert stopword string to list & append additional stopword\n",
    "    list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "\n",
    "    # convert list to dictionary\n",
    "    list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "    #remove stopword pada list token\n",
    "    def stopwords_removal(words):\n",
    "        return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "    TWEET_DATA['tweet_tokens_WSW'] = TWEET_DATA['tweet_tokens'].apply(stopwords_removal) \n",
    "    \n",
    "    # import Sastrawi package\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    import swifter\n",
    "\n",
    "\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    # stemmed\n",
    "    def stemmed_wrapper(term):\n",
    "        return stemmer.stem(term)\n",
    "\n",
    "    term_dict = {}\n",
    "\n",
    "    for document in TWEET_DATA['tweet_tokens_WSW']:\n",
    "        for term in document:\n",
    "            if term not in term_dict:\n",
    "                term_dict[term] = ' '\n",
    "\n",
    "    print(len(term_dict))\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    for term in term_dict:\n",
    "        term_dict[term] = stemmed_wrapper(term)\n",
    "        print(term,\":\" ,term_dict[term])\n",
    "\n",
    "    print(term_dict)\n",
    "    print(\"------------------------\")\n",
    "\n",
    "\n",
    "    # apply stemmed term to dataframe\n",
    "    def get_stemmed_term(document):\n",
    "        return [term_dict[term] for term in document]\n",
    "\n",
    "    TWEET_DATA['tweet_tokens_stemmed'] = TWEET_DATA['tweet_tokens_WSW'].swifter.apply(get_stemmed_term)\n",
    "    \n",
    "    TWEET_DATA.to_csv(\"Text_Preprocessing.csv\")\n",
    "    \n",
    "    return \"preprocessing berhasil\"\n",
    "\n",
    "\n",
    "# API TF-IDF\n",
    "@app.route('/tf-idf')\n",
    "def tfidf():\n",
    "    TWEET_DATA = pd.read_csv(\"Text_Preprocessing.csv\", usecols=[\"tweet_tokens_stemmed\"])\n",
    "    TWEET_DATA.columns = [\"tweet\"]\n",
    "\n",
    "    def convert_text_list(texts):\n",
    "        texts = ast.literal_eval(texts)\n",
    "        return [text for text in texts]\n",
    "\n",
    "    TWEET_DATA[\"tweet_list\"] = TWEET_DATA[\"tweet\"].apply(convert_text_list)\n",
    "    \n",
    "    def calc_TF(document):\n",
    "        # Counts the number of times the word appears in review\n",
    "        TF_dict = {}\n",
    "        for term in document:\n",
    "            if term in TF_dict:\n",
    "                TF_dict[term] += 1\n",
    "            else:\n",
    "                TF_dict[term] = 1\n",
    "        # Computes tf for each word\n",
    "        for term in TF_dict:\n",
    "            TF_dict[term] = TF_dict[term] / len(document)\n",
    "        return TF_dict\n",
    "\n",
    "    TWEET_DATA[\"TF_dict\"] = TWEET_DATA['tweet_list'].apply(calc_TF)\n",
    "    \n",
    "    # Check TF result\n",
    "    #index = 99\n",
    "    index = TWEET_DATA.shape[0]\n",
    "    \n",
    "    print('%20s' % \"term\", \"\\t\", \"TF\\n\")\n",
    "    for key in TWEET_DATA[\"TF_dict\"][index-1]:\n",
    "        print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index-1][key])\n",
    "    \n",
    "    def calc_DF(tfDict):\n",
    "        count_DF = {}\n",
    "        # Run through each document's tf dictionary and increment countDict's (term, doc) pair\n",
    "        for document in tfDict:\n",
    "            for term in document:\n",
    "                if term in count_DF:\n",
    "                    count_DF[term] += 1\n",
    "                else:\n",
    "                    count_DF[term] = 1\n",
    "        return count_DF\n",
    "\n",
    "    DF = calc_DF(TWEET_DATA[\"TF_dict\"])\n",
    "    \n",
    "    n_document = len(TWEET_DATA)\n",
    "\n",
    "    def calc_IDF(__n_document, __DF):\n",
    "        IDF_Dict = {}\n",
    "        for term in __DF:\n",
    "            IDF_Dict[term] = np.log(__n_document / (__DF[term] + 1))\n",
    "        return IDF_Dict\n",
    "\n",
    "    #Stores the idf dictionary\n",
    "    IDF = calc_IDF(n_document, DF)\n",
    "    \n",
    "    #calc TF-IDF\n",
    "    def calc_TF_IDF(TF):\n",
    "        TF_IDF_Dict = {}\n",
    "        #For each word in the review, we multiply its tf and its idf.\n",
    "        for key in TF:\n",
    "            TF_IDF_Dict[key] = TF[key] * IDF[key]\n",
    "        return TF_IDF_Dict\n",
    "\n",
    "    #Stores the TF-IDF Series\n",
    "    TWEET_DATA[\"TF-IDF_dict\"] = TWEET_DATA[\"TF_dict\"].apply(calc_TF_IDF)\n",
    "    \n",
    "    # Check TF-IDF result\n",
    "    #index = 99\n",
    "    index = TWEET_DATA.shape[0]\n",
    "\n",
    "    print('%20s' % \"term\", \"\\t\", '%10s' % \"TF\", \"\\t\", '%20s' % \"TF-IDF\\n\")\n",
    "    for key in TWEET_DATA[\"TF-IDF_dict\"][index-1]:\n",
    "        print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index-1][key] ,\"\\t\" , TWEET_DATA[\"TF-IDF_dict\"][index-1][key])\n",
    "        \n",
    "    \n",
    "    # sort descending by value for DF dictionary \n",
    "    sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)[:100]\n",
    "\n",
    "    # Create a list of unique words from sorted dictionay `sorted_DF`\n",
    "    unique_term = [item[0] for item in sorted_DF]\n",
    "\n",
    "    def calc_TF_IDF_Vec(__TF_IDF_Dict):\n",
    "        TF_IDF_vector = [0.0] * len(unique_term)\n",
    "\n",
    "        # For each unique word, if it is in the review, store its TF-IDF value.\n",
    "        for i, term in enumerate(unique_term):\n",
    "            if term in __TF_IDF_Dict:\n",
    "                TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "        return TF_IDF_vector\n",
    "\n",
    "    TWEET_DATA[\"TF_IDF_Vec\"] = TWEET_DATA[\"TF-IDF_dict\"].apply(calc_TF_IDF_Vec)\n",
    "    \n",
    "    \n",
    "    # Convert Series to List\n",
    "    TF_IDF_Vec_List = np.array(TWEET_DATA[\"TF_IDF_Vec\"].to_list())\n",
    "\n",
    "    # Sum element vector in axis=0 \n",
    "    sums = TF_IDF_Vec_List.sum(axis=0)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for col, term in enumerate(unique_term):\n",
    "        data.append((term, sums[col]))\n",
    "\n",
    "    ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "    ranking.sort_values('rank', ascending=False)\n",
    "    \n",
    "    # join list of token as single document string\n",
    "\n",
    "    def join_text_list(texts):\n",
    "        texts = ast.literal_eval(texts)\n",
    "        return ' '.join([text for text in texts])\n",
    "    \n",
    "    TWEET_DATA[\"tweet_join\"] = TWEET_DATA[\"tweet\"].apply(join_text_list)\n",
    "    \n",
    "#     from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#     from sklearn.preprocessing import normalize\n",
    "\n",
    "    max_features = 100\n",
    "\n",
    "    # calc TF vector\n",
    "    cvect = CountVectorizer(max_features=max_features)\n",
    "    TF_vector = cvect.fit_transform(TWEET_DATA[\"tweet_join\"])\n",
    "\n",
    "    # normalize TF vector\n",
    "    normalized_TF_vector = normalize(TF_vector, norm='l1', axis=1)\n",
    "\n",
    "    # calc IDF\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, smooth_idf=False)\n",
    "    tfs = tfidf.fit_transform(TWEET_DATA[\"tweet_join\"])\n",
    "    IDF_vector = tfidf.idf_\n",
    "\n",
    "    # hitung TF x IDF sehingga dihasilkan TFIDF matrix / vector\n",
    "    tfidf_mat = normalized_TF_vector.multiply(IDF_vector).toarray()\n",
    "\n",
    "    max_features = 100\n",
    "\n",
    "    # ngram_range (1, 3) to use unigram, bigram, trigram\n",
    "    cvect = CountVectorizer(max_features=max_features, ngram_range=(1,3))\n",
    "    counts = cvect.fit_transform(TWEET_DATA[\"tweet_join\"])\n",
    "\n",
    "    normalized_counts = normalize(counts, norm='l1', axis=1)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1,3), smooth_idf=False)\n",
    "    tfs = tfidf.fit_transform(TWEET_DATA[\"tweet_join\"])\n",
    "\n",
    "    # tfidf_mat = normalized_counts.multiply(tfidf.idf_).toarray()\n",
    "    \n",
    "    type(counts)\n",
    "    counts.shape\n",
    "    \n",
    "    print(tfidf.vocabulary_)\n",
    "    \n",
    "    print(tfidf.get_feature_names_out())\n",
    "    a=tfidf.get_feature_names_out()\n",
    "    \n",
    "    print(tfs.toarray())\n",
    "    b=tfs.toarray()\n",
    "    \n",
    "    tfidf_mat = normalized_counts.multiply(IDF_vector).toarray()\n",
    "    dfbtf = pd.DataFrame(data=tfidf_mat,columns=[a])\n",
    "    dfbtf\n",
    "    \n",
    "    dfbtf.to_csv(\"hasil_vector_matrix.csv\")\n",
    "    \n",
    "    return \"tf-idf sukses\"\n",
    "\n",
    "# API Pelabelan sentimen\n",
    "@app.route('/sentimen-pelabelan')\n",
    "def pelabelan():\n",
    "\n",
    "    # Baca file excel\n",
    "    data = pd.read_csv('hasil_vector_matrix.csv')\n",
    "    data1 = pd.read_csv('data_tweet.csv')\n",
    "    # Mengambil indeks baris dengan status = 0\n",
    "    index_zero_status = data1.loc[data1['status'] == 0].index.tolist()\n",
    "    \n",
    "    threshold = 0.5\n",
    "    # mengambil nilai pada kolom terakhir pada setiap baris data, kecuali kolom terakhir yang merupakan label sentimen\n",
    "    last_row = data.iloc[:, -1]\n",
    "\n",
    "    # list untuk menyimpan label sentimen\n",
    "    sentiments = []\n",
    "\n",
    "    # loop pada setiap nilai pada kolom terakhir pada setiap baris data\n",
    "    for i in range(len(last_row)):\n",
    "        if last_row[i] >= threshold:\n",
    "            # jika nilai lebih besar atau sama dengan threshold, sentimen positif\n",
    "            sentiments.append('positif')\n",
    "        else:\n",
    "            # jika nilai kurang dari threshold, sentimen negatif\n",
    "            sentiments.append('negatif')\n",
    "\n",
    "    # menambahkan kolom \"sentimen\" ke dalam dataframe\n",
    "    data.loc[data.index.isin(index_zero_status), 'status'] = 0\n",
    "    data.loc[~data.index.isin(index_zero_status), 'status'] = 1\n",
    "    \n",
    "    data['aktual'] = sentiments\n",
    "    data['sentimen'] = sentiments\n",
    "    data.loc[data['status'] == 0, 'aktual'] = '-'\n",
    "    \n",
    "    data.to_csv(\"pelabelan.csv\", index=False)\n",
    "\n",
    "    # Tampilkan dataframe hasil pelabelan\n",
    "    print(data)\n",
    "    last_row = data.tail(1)\n",
    "    data = last_row.to_dict(orient='records')\n",
    "    return jsonify(data)\n",
    "    \n",
    "#similarity\n",
    "@app.route('/similarity', methods=['GET'])\n",
    "def similarity():\n",
    "    # Membaca data vektor dan sentimen dari file CSV\n",
    "    df_vectors = pd.read_csv(\"pelabelan.csv\", header=None, skiprows=1)\n",
    "    vectors = df_vectors.iloc[:, 1:-3].values\n",
    "    sentiments = df_vectors.iloc[:, -1].values\n",
    "\n",
    "    # Membaca data tweet dari file CSV\n",
    "    df_tweet = pd.read_csv(\"data_tweet.csv\", encoding='latin1')\n",
    "\n",
    "    # Mengambil data input yang akan dibandingkan dengan semua data\n",
    "    input_vector_sentiments = vectors[-1]\n",
    "\n",
    "    # Menghitung cosine similarity\n",
    "    similarities_sentiments = cosine_similarity([input_vector_sentiments], vectors[:-1])[0]\n",
    "\n",
    "    # Mengambil 3 data dengan nilai cosine similarity tertinggi\n",
    "    top_indices_sentiments = np.argsort(similarities_sentiments)[-3:][::-1]\n",
    "\n",
    "    # Menyimpan hasil ranking data tertinggi dan teks tweet\n",
    "    results = []\n",
    "    for i, idx in enumerate(top_indices_sentiments):\n",
    "        rank = i + 1\n",
    "        similarity = similarities_sentiments[idx]\n",
    "        data_number = int(idx + 1)\n",
    "        tweet_text = df_tweet.iloc[idx]['rawContent']\n",
    "        result = {\n",
    "            \"rank\": rank,\n",
    "            \"data_number\": data_number,\n",
    "            \"cosine_similarity\": float(similarity),\n",
    "            \"tweet\": tweet_text\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Menghitung jumlah sentimen negatif dan positif dari 3 data ranking teratas\n",
    "    top_sentiments = sentiments[top_indices_sentiments]\n",
    "    num_negative_sentiments = np.sum(top_sentiments == 'negatif')\n",
    "    num_positive_sentiments = np.sum(top_sentiments == 'positif')\n",
    "\n",
    "    # Menentukan sentimen berdasarkan perbandingan jumlah sentimen positif dan negatif\n",
    "    if num_positive_sentiments > num_negative_sentiments:\n",
    "        sentiment_label = \"positif\"\n",
    "    else:\n",
    "        sentiment_label = \"negatif\"\n",
    "\n",
    "    # Menghitung accuracy positif dan accuracy negatif\n",
    "    accuracy_positif = 0\n",
    "    accuracy_negatif = 0\n",
    "\n",
    "    if num_positive_sentiments > 0:\n",
    "        max_positive_similarity = np.max(similarities_sentiments[top_indices_sentiments[top_sentiments == 'positif'][:3]])\n",
    "        accuracy_positif = float(max_positive_similarity) if max_positive_similarity > 0 else 0\n",
    "\n",
    "    if num_negative_sentiments > 0:\n",
    "        max_negative_similarity = np.max(similarities_sentiments[top_indices_sentiments[top_sentiments == 'negatif'][:3]])\n",
    "        accuracy_negatif = float(max_negative_similarity) if max_negative_similarity > 0 else 0\n",
    "\n",
    "    # Menyimpan hasil\n",
    "    response = {\n",
    "        \"sentiment\": sentiment_label,\n",
    "        \"accuracy_positif\": accuracy_positif,\n",
    "        \"accuracy_negatif\": accuracy_negatif,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "    return jsonify(response)\n",
    "\n",
    "# approve status\n",
    "@app.route('/approve-status', methods=['POST'])\n",
    "def approve_status():\n",
    "    # Mendapatkan indeks yang dikirim melalui body request\n",
    "    index = request.json.get('index')\n",
    "\n",
    "    # Update status pada file data_tweet.csv\n",
    "    tweet_data = pd.read_csv('data_tweet.csv')\n",
    "    if int(index) in tweet_data.index:\n",
    "        tweet_data.at[int(index), 'status'] = 1\n",
    "        tweet_data.to_csv('data_tweet.csv', index=False)\n",
    "    else:\n",
    "        return jsonify({'message': 'Invalid index for data_tweet.csv.'})\n",
    "\n",
    "    # Update status pada file pelabelan.csv\n",
    "    pelabelan_data = pd.read_csv('pelabelan.csv')\n",
    "    if int(index) in pelabelan_data.index:\n",
    "        pelabelan_data.at[int(index), 'status'] = 1\n",
    "        pelabelan_data.at[int(index), 'aktual'] = pelabelan_data.at[int(index), 'sentimen']  # Menambahkan pembaruan nilai 'aktual' dengan nilai 'sentimen'\n",
    "        pelabelan_data.to_csv('pelabelan.csv', index=False)\n",
    "    else:\n",
    "        return jsonify({'message': 'Invalid index for pelabelan.csv.'})\n",
    "\n",
    "    return jsonify({'message': 'Status updated successfully.'})\n",
    "\n",
    "\n",
    "# decline status\n",
    "@app.route('/decline-status', methods=['POST'])\n",
    "def decline_status():\n",
    "    # Mendapatkan indeks yang dikirim melalui body request\n",
    "    index = request.json.get('index')\n",
    "\n",
    "    # Update status pada file data_tweet.csv\n",
    "    tweet_data = pd.read_csv('data_tweet.csv')\n",
    "    if int(index) in tweet_data.index:\n",
    "        tweet_data.at[int(index), 'status'] = 2\n",
    "        tweet_data.to_csv('data_tweet.csv', index=False)\n",
    "    else:\n",
    "        return jsonify({'message': 'Invalid index for data_tweet.csv.'})\n",
    "\n",
    "   # Update status pada file pelabelan.csv\n",
    "    pelabelan_data = pd.read_csv('pelabelan.csv')\n",
    "    if int(index) in pelabelan_data.index:\n",
    "        pelabelan_data.at[int(index), 'status'] = 2\n",
    "\n",
    "        sentimen = pelabelan_data.at[int(index), 'sentimen']\n",
    "        aktual = 'negatif' if sentimen == 'positif' else 'positif'\n",
    "        pelabelan_data.at[int(index), 'aktual'] = aktual\n",
    "\n",
    "        pelabelan_data.to_csv('pelabelan.csv', index=False)\n",
    "    else:\n",
    "        return jsonify({'message': 'Invalid index for pelabelan.csv.'})\n",
    "\n",
    "    return jsonify({'message': 'Status updated successfully.'})\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091c390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0baeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
